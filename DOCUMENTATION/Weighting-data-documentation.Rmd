---
title: "Weighting survey data to adjust for demographically non-representative samples"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Problem Statement

To standardize and develop norms for behavior rating scale and performance tests, we collect nationwide samples of test performance data. These include data from many thousands of persons. Our intent is to have the demographic makeup of these samples match the U.S. census data, as closely as possible. The demographic variables we care about are age, gender, (parent) education level, race/ethnicity, and U.S. geographic region.

The most rigorous approach we use is to stratify the sample by age, and ensure that we have enough cases within each age group to develop stable norms. Within each age strata, we attempt to match the proportions of gender, education level, race/ethnicity, and region to the census proportions for that age group. Inevitably, some demographic cells are harder to fill than others, especially lower levels of education and certain ethnic minority groups. Pursuing the last few cases to fill out these cells is expensive and time-consuming.

We are interested in weighting the data of these samples that fall short of the ideal demographic composition. What I mean by this is to calculate weighing multipliers corresponding to each possible crossing of the four demographic variables of interest. For example, one crossing would be female, high-school diploma, hispanic, south region. All cases falling into that particular bucket would have the same weighting multiplier.

That multiplier would be applied to the test response data at the most granular level. For example, we can imagine a behavior rating scale where a parent answers 50 questions about whether their child has shown hyperactive or inattentive behaviors over the past month. These behaviors would be rated on a frequency scale, coded as follows:

* Never (or almost never) = 1
* Occasionally = 2
* Frequently = 3
* Always (or almost always) = 4

Because all behaviors being rated are problematic, a higher code on any item indicates greater severity of ADHD-related symptoms.  For simplicity’s sake let us suppose that the item codes from these 50 items are summed to yield a single score that represents the severity of these symptoms.

What I envision is that the demographic weighting multiplier would be applied to each of the 50 item codes, for each person. These data would then be weighted for all subsequent item- and score-level analyses.

How this would work in practice would be as follows. Let’s suppose that the previously mentioned demographic cell (female, high-school, hispanic, south) was under-represented in our sample with respect to the U.S. census proportions for that class. The demographic weighting multiplier would serve the purpose of increasing the influence of an insufficient number of cases in this cell, on the sample as a whole. In other words, if we collected 50 cases in this cell, and the census required 70,  the demographic weighting multiplier would serve to increase the individual item codes ratings our 50 cases. After the multiplier is applied, summing the item ratings from from our 50 cases would yield a total score across these cases numerically equivalent to the sum of the item ratings of 70 non-weighted cases with the same demographic characteristics. And this is the manner is which this weighting would adjust the scores of our sample so that the sample statistics would be similar to those obtained from a sample that exactly matched the U.S. Census demographic proportions.

At this point I have to pause and ask you to check my logic. Am I thinking about the problem and its solution in the right way? Is the application of case-wise demographic weights, in the manner that I’ve described, the correct method for adjusting the statistics of a sample that falls short of demographic representativeness.


### Work to date with Survey package

First we read in a simulated data set that I created with the `psych` package.
```{r read-data, eval=FALSE}
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(survey)))
set.seed(123)

urlRemote_path  <- "https://raw.githubusercontent.com/"
github_path <- "DSHerzberg/WEIGHTING-DATA/master/INPUT-FILES/"
fileName_path   <- "unweighted-input.csv"

unweighted_input <- suppressMessages(read_csv(url(
  str_c(urlRemote_path, github_path, fileName_path)
)))
```

###### COMMENTED SNIPPETS
First load three required packages.

In this example, we have raw-to-standard score lookup tables for two scales (`COG`, `EMO`), in the required `.xlsx` input structure for this procecedure. These lookup relationships are stratified by age, with the lookup table for each `agestrat` contained in a separate tab of the `.xlsx`. The lookup relationships are further stratified by test form, with separate `.xlsx` files for `parent`, `teacher`, and `self` forms.

The three `.xlsx` files are named using the standard convention:

`[file level variable value]-[tab level variable name]-[lookup input variable name]To[lookup output variable name].xlsx`

For example:

`parent-agestrat-rawToSS.xlsx`

The generic code for reading an input file with stratification over file, tab, and table levels requries the user to designate token values. For the file-level variable, you must specify both the name (e.g., `'form'`) and the values (e.g., `c('parent', 'teacher', 'self')`). The table level of stratification captures the core lookup relationship between two variables:

* `key_var`: the lookup input (e.g., `raw` score), located in the table's far-left column; and,
* `col_var`: the lookup output (e.g., `SS`, or standard score), located in the columns to the right of the key column.
```{r three_level_readin, echo=1:9, eval=FALSE}
```
Here we write a function `three_level_readin()` to read in the input file. We use `here::here()` and `base::paste0` to express the file path to the input file as a character string. Note the token substitutions in the string. `base::assign()` is used to assign the string to the object `path` in the global environment, so that it can be accessed from there when `three_level_readin()` is called.
```{r three_level_readin, echo=11:14, eval=FALSE}
```
Next we pipe the directory path to the input file through functions from the `readxl` package. `readxl::excel_sheets()` extracts the tab labels of an `.xlsx` file into a character vector. `purrr::set_names()` gives each element of the vector a name that is identical to that element (e.g., `var` is named "var").

`purrr::map_df()` applies the function `readxl::read_excel()` to each element of this named vector (i.e., to each tab, or sheet, of the multi-tab input `.xlsx`), and returns a data frame as its output. The `path` argument points to the input `.xlsx`, and the sheets of this input are read consecutively and stacked on top of one another in the output data frame. The `.id` argument prepends a new column, named with the `tab_strat_var` token, that identifies the origin sheet of each set of rows in the output data frame. 
```{r three_level_readin, echo=15:22, eval=FALSE}
```
In the next snippet, `three_level_readin()` is called to read the three input `.xlsx` files into a single data frame. This is accomplished by calling `map()` to apply `three_level_readin()` iteratively to the three values of the file level varible, contained in the character vector `file_strat_var`. This returns three data frames as described in the previous section.

These three objects are then named appropriately using `stats::setNames()`. They are stacked on top of one another and bound into a single data frame with `dplyr::bind_rows()`. The `.id` argument prepends a new column identifying the origin `.xlsx` for each set of rows, according to the appropriate value of `file_strat_var`. The resulting object is `assign()`ed to the global environment, with a name corresponding to the input file conventions, with `_lookup` appended.
```{r three_level_readin, echo=24:30, eval=FALSE}
```
At this point, the data object has the following column structure:

form | agestrat | rawscore | COG | EMO
-----| ---------| ---------| ----| ---
parent | 060 | 1 | 89 | 87

This is close to the foundational structure needed to assemble the final OES lookup table, but a further transformation is required. In the final OES table, all lookup input variables must be present in a tall, multilevel (nested) format. In the current data object, the columns `form`, `agestrat`, and `rawscore` are in this format. All possible values of `rawscore` are nested within each value of `agestrat`, and, in turn, all values of `agestrat` are nested within each value of `form`.


The columns `COG` and `EMO` contain SS values corresponding to each value of `rawscore`. These columns combine two key foundational lookup inputs: `scale` and `SS`. In the final OES table, the `scale` and `SS` variables must have their own columns, and values must be arranged in the tall, multilevel format. In essence, the raw-to-ss correspondence now captured in the `COG` and `EMO` columns must be nested with respect to a `scale` column in which `COG` and `EMO` are row values instead of column names.

The next snippet accomplishes this transformation using `tidyr::gather()`. The transformed object is named `lookup_foundation` to denote its foundational status vis-a-vis the desired final OES table.

In this call of `gather()`, `'scale'` and `'SS'` name the new columns that will hold the information now contained in the `COG` and `EMO` columns of the current data object. The `scale` column will hold the scale names `COG` and `EMO` (currently represented as column names). The `SS` column will hold the values of SS for each scale. The expression `-form, -agestrat, -rawscore` removes these three columns from the gathering operation, so that the only columns transformed are `COG` and `EMO`. In fact, the within-row relationship between `form`, `agestrat`, `rawscore` and SS that exists in the current object will be preserved in the transformed object, but it will be replicated for each value of `scale`, going down the rows of the transformed table. The transformed table thus is formatted as follows:

scale | form | agestrat | rawscore | SS
-----| ---------| ---------| ----| ---
COG | parent | 060 | 1 | 89
EMO | parent | 060 | 1 | 87

In the transformed table, all five foundational lookup inputs (`scale`, `form`, `agestrat`, `rawscore`, `SS`) are each represented in their own column, which in turn contains all possible values of that input. The structure is fully nested, meaning that every possible crossing of the lookup inputs is represented in a unique row.

This structure simplifies the process of adding lookup output columns to the table. To add a column, you create an input object that captures the lookup relationship between input and output (e.g., an `SS` to `age_equiv` lookup), and join that input object to `lookup_foundation`.

```{r three_level_readin, echo=32:34, eval=FALSE}
```
