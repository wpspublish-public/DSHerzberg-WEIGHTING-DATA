---
title: "Weighting behavior rating data to adjust for demographically non-representative samples"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Problem Statement

To standardize and develop norms for behavior rating scale and performance tests, we collect nationwide samples of test performance data. These include data from many thousands of persons. Our intent is to have the demographic makeup of these samples match the U.S. census data, as closely as possible. The demographic variables we care about are age, gender, (parent) education level, race/ethnicity, and U.S. geographic region.

The most rigorous approach we use is to stratify the sample by age, and ensure that we have enough cases within each age group to develop stable norms. Within each age strata, we attempt to match the proportions of gender, education level, race/ethnicity, and region to the census proportions for that age group. Inevitably, some demographic cells are harder to fill than others, especially lower levels of education and certain ethnic minority groups. Pursuing the last few cases to fill out these cells is expensive and time-consuming.

We are interested in weighting the data of these samples that fall short of the ideal demographic composition. What I mean by this is to calculate weighing multipliers corresponding to each possible crossing of the four demographic variables of interest. For example, one crossing would be female, high-school diploma, hispanic, south region. All cases falling into that particular bucket would have the same weighting multiplier.

That multiplier would be applied to the test response data at the most granular level. For example, we can imagine a behavior rating scale where a parent answers 50 questions about whether their child has shown hyperactive or inattentive behaviors over the past month. These behaviors would be rated on a frequency scale, coded as follows:

* Never (or almost never) = 1
* Occasionally = 2
* Frequently = 3
* Always (or almost always) = 4

Because all behaviors being rated are problematic, a higher code on any item indicates greater severity of ADHD-related symptoms.  For simplicity’s sake let us suppose that the item codes from these 50 items are summed to yield a single score that represents the severity of these symptoms.

What I envision is that the demographic weighting multiplier would be applied to each of the 50 item codes, for each person. These data would then be weighted for all subsequent item- and score-level analyses.

How this would work in practice would be as follows. Let’s suppose that the previously mentioned demographic cell (female, high-school, hispanic, south) was under-represented in our sample with respect to the U.S. census proportions for that class. The demographic weighting multiplier would serve the purpose of increasing the influence of an insufficient number of cases in this cell, on the sample as a whole. In other words, if we collected 50 cases in this cell, and the census required 70,  the demographic weighting multiplier would serve to increase the individual item codes ratings our 50 cases. After the multiplier is applied, summing the item ratings from from our 50 cases would yield a total score across these cases numerically equivalent to the sum of the item ratings of 70 non-weighted cases with the same demographic characteristics. And this is the manner is which this weighting would adjust the scores of our sample so that the sample statistics would be similar to those obtained from a sample that exactly matched the U.S. Census demographic proportions.

At this point I have to pause and ask you to check my logic. Am I thinking about the problem and its solution in the right way? Is the application of case-wise demographic weights, in the manner that I’ve described, the correct method for adjusting the statistics of a sample that falls short of demographic representativeness.


### Work to date with Survey package

#### Read unweighted input data

First we read in a simulated data set that I created with the `psych` package. The data set includes 1000 cases with the four demographic variables of interest, and 50 behavior ratings items coded as in the example above. (Note: if you are unable to read the data set from the remote URL, I can provide code that will enable you to create the data.)
```{r read-data, eval=FALSE}
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(survey)))
set.seed(123)

urlRemote_path  <- "https://raw.githubusercontent.com/"
github_path <- "DSHerzberg/WEIGHTING-DATA/master/INPUT-FILES/"
fileName_path   <- "unweighted-input.csv"

unweighted_input <- suppressMessages(read_csv(url(
  str_c(urlRemote_path, github_path, fileName_path)
)))
```

<br>

#### Compare demographic counts to census targets

Running the next code chunk yields a table `freq_demos_comp` that compares the demographic counts of the unweighted input to the target proportions of the U.S. Census (which are captured in the vectors with the suffix `_census`). `freq_demos_comp` is sorted by the right-most column `diff_pct`, which captures the difference in the percentage of a demographic category between the unweighted input and the census target. 

The table shows, for example, that the `female-no_HS_hispanic_northeast` cell is the most under-represented cell in the input, whereas the `male-BA_plus-white-midwest` cell is the most over-represented. By contrast, the input sample is closer to the census proportions for blacks or asians of either gender from the `south` or `west`, with educational levels of `HS_grad` or  `some_college`.

By the logic expressed previously, I would therefore expect to require a demographic weighting multiplier greater than 1 for cases in the `female-no_HS_hispanic_northeast` cell, and a multiplier less than 1 for cases in the `male-BA_plus-white-midwest` cell.
```{r demo-counts, eval=FALSE}
var_order <- c("age", "age_range", "gender", "educ", "ethnic", "region", "clin_status")

cat_order <- c(
  # age
  NA, "5", "6", "7", "8", "9", "10", "11", "12",
  # age_range
  NA, "5 to 8 yo", "9 to 12 yo", 
  # Gender
  NA, "male", "female",
  # educ
  NA, "no_HS", "HS_grad", "some_college", "BA_plus",
  # Ethnicity
  NA, "hispanic", "asian", "black", "white", "other",
  # Region
  NA, "northeast", "south", "midwest", "west")

gender_census <- tibble(cat = c("female", "male"),
                        Freq = nrow(unweighted_input)*c(0.53, 0.47))
educ_census <- tibble(cat = c("no_HS", "HS_grad", "some_college", "BA_plus"),
                      Freq = nrow(unweighted_input)*c(0.119, 0.263, 0.306, 0.311))
ethnic_census <- tibble(cat = c("hispanic", "asian", "black", "white", "other"),
                        Freq = nrow(unweighted_input)*c(0.239, 0.048, 0.136, 0.521, .056))
region_census <- tibble(cat = c("northeast", "south", "midwest", "west"),
                        Freq = nrow(unweighted_input)*c(0.166, 0.383, 0.212, 0.238))

freq_demos_unweighted <- unweighted_input %>%
  pivot_longer(age_range:clin_status, names_to = 'var', values_to = 'cat') %>%
  group_by(var, cat) %>%
  count(var, cat) %>%
  arrange(match(var, var_order), match(cat, cat_order)) %>%
  ungroup() %>%
  mutate(
   pct_samp = round(((n / nrow(unweighted_input)) * 100), 1)
  ) %>%
  select(var, cat, n, pct_samp) %>% 
  full_join(region_census, by = "cat")

list_demos <- list(freq_demos_unweighted, gender_census, educ_census, 
                   ethnic_census, region_census)

freq_demos_comp <- list_demos %>% 
  reduce(left_join, by = "cat") %>% 
  filter(!(var %in% c("age_range", "clin_status"))) %>% 
  unite(census_count, c(Freq.x, Freq.y, Freq.x.x, Freq.y.y), sep = "", remove = T) %>% 
  mutate_at(vars(census_count), ~as.integer(str_replace_all(., "NA", ""))) %>% 
  mutate(census_pct = 100 * round(census_count/nrow(unweighted_input), 3),
         diff_pct = census_pct - pct_samp
         ) %>% 
  rename(input_count = n, input_pct = pct_samp) %>% 
  select(var, cat, input_count, census_count, input_pct, census_pct, diff_pct) %>% 
  arrange(desc(diff_pct))

rm(list = setdiff(ls(), ls(pattern = "input|comp|list")))

list_census <- list(list_demos[2:5],
     c("gender", "educ", "ethnic", "region")) %>%
  pmap(~ ..1 %>%
         rename_at(vars(cat), ~str_replace(., "cat", !!..2))
  )

names(list_census) <- c("gender_census", "educ_census", "ethnic_census", "region_census")

list2env(list_census, envir=.GlobalEnv)
```

<br>

#### Implement survey functions

We now use functions from the Survey package to first create a survey object based on the unweighted input data, then to rake that object against the census targets for `gender`, `educ`, `ethnic`, and `region`. In examining the resulting `rake_unweighted_input` object, I found the `prob` vector, which appears to contain the case-wise weights generated by the raking procedure. I then bound that vector to input in the data frame `input_demo_wts`, and sorted it by the `demo_wt` column.
```{r survey-funs, eval=FALSE}
unweighted_survey_object <- svydesign(ids = ~1, 
                                     data = unweighted_input, 
                                     weights = NULL)

rake_unweighted_input <- rake(design = unweighted_survey_object,
                          sample.margins = list(~gender, ~educ, ~ethnic, ~region),
                          population.margins = list(gender_census, educ_census, 
                                                    ethnic_census, region_census))

input_demo_wts <- bind_cols(
  rake_unweighted_input[["variables"]], 
  data.frame(rake_unweighted_input[["prob"]])
  ) %>% 
  rename(demo_wt = rake_unweighted_input...prob...) %>% 
  select(ID:clin_status, demo_wt, everything()) %>% 
  arrange(desc(demo_wt))

tail(input_demo_wts)

filter(input_demo_wts, between(demo_wt, .98, 1.02))

head(input_demo_wts)
```
When I examine the values in the `demo_wt` column, I see that they are related to the demographic classifications of the rows in the way that I expected them to be. That is, for example, the `demo_wt` value of `0.163` associated with the the `female-no_HS_hispanic_northeast` classification is the relatively distant from one, as we can see from calling `tail(input_demo_wts)`.

When we examine the values of `demo_wt` clustered around 1 by calling `filter(input_demo_wts, between(demo_wt, .98, 1.02))`,  we see primarily the expected demographic categories of `HS_grad`, `black`, and `south`.

Here is where I run up against the limit of my understanding of the Survey functions and the output of `rake()`. Because the `female-no_HS_hispanic_northeast` cell is under-sampled relative to the census target, I would expect the `demo_wt` value to be greater than 1, by the logic I described previously. That is, the impact of the existing cases in that cell on the sample statistics would have to be magnified by a multiplier greater than 1, to offset the fact that there are fewer cases in that cell than would be dictated by the census targets.

The fact that `demo_wt` is __less__ than 1 for `female-no_HS_hispanic_northeast` this cell is counter-intuitive to me. It seems like an inversion of sorts of what the value should be. And that inversion phenomenon is present throughout the rows of `input_demo_wts`. Calling `head(input_demo_wts)` reveals cases from the over-sampled crossing of `BA_plus` and `midwest` with a `demo_wt` value of `2.167`. This again is counter-intuitive; I would expect a `demo_wt` less than 1, to reduce the impact of these over-sampled categories on the sample statistics.

I can only conclude, then, that the values of the `prob` vector are in fact the weights associated with the __input__ demographic distribation, and they in fact express the level of under-sampling or oversampling of any demographic cell. They cannot be used as demographic weighting multipliers to adjust the input sample distributions, and so these values cannot fulfill the purpose that I set up to fulfill in the Problem Statement.

<br>
