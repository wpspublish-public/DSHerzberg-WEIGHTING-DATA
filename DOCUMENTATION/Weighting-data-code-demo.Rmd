---
title: "Weighting data to adjust for demographically non-representative samples"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

### I. Overview

To develop norms for tests and behavior rating scales, we collect nationwide samples comprised of data from many thousands of persons. Our intent is to have the demographic makeup of these samples match the U.S. census proportions, as closely as possible. The demographic variables of interest are age, gender, (parent) education level, race/ethnicity, and U.S. geographic region.

Our approach is to stratify the sample by age, and ensure that we have enough cases within each age group to develop stable norms. Within each age stratum, we attempt to match the proportions of gender, education level, race/ethnicity, and region to the census proportions for that age group. Inevitably, some demographic cells are harder to fill than others, especially lower levels of education and certain ethnic minority groups. Pursuing the last few cases to fill out these cells is expensive and time-consuming.

We now have the option of weighting the data of samples that fall short of the ideal demographic composition. We can calculate weighting multipliers corresponding to each possible crossing of the categories within the four key demographic variables. For example, one crossing would be `female X high-school diploma X hispanic X south region`. All cases falling into this cell would have the same weighting multiplier.

We can apply that multiplier to the data at the most granular level: individual item scores. As an example, we can use a hypothetical behavior rating scale where a parent rates 50 items about their child's level of hyperactive/inattentive behavior during the past month. These behaviors are rated on a frequency scale, coded as follows:

* Never (or almost never) = 1
* Occasionally = 2
* Frequently = 3
* Always (or almost always) = 4

A higher code on any item indicates greater frequency of problematic behavior.

We can apply a demographic weighting multiplier on a case-wise basis to each of the 50 item codes. These data would then be weighted for all subsequent item- and score-level analyses.

In practice this would work as follows. Suppose that the previously mentioned demographic cell (`female X high-school diploma X hispanic X south region`) was under-represented in our sample, with respect to U.S. census proportions. The demographic weighting multiplier would serve to increase the numerical impact of the insufficient number of cases in this cell. Thus, if we collected 40 cases in this cell, and the census required 60, the multiplier would be applied to increase the values of the item codes in our 40 cases. Once these values were increased, summing the item codes from from our 40 cases would yield a sum approximately equivalent to the sum of the item codes of 60 non-weighted cases with the same demographic characteristics. 

To recount, we can use this weighting method to adjust the values of the data in our collected sample, so that the sample statistics would be similar to those obtained from a sample that exactly matched the U.S. Census demographic proportions.

### II. Demonstration of Method

#### A. Load packages, specify input parameters, read data

We read in two simulated data sets, each including 1000 cases with the four demographic variables of interest, and 50 behavior rating items coded as described above. 

* `original_input` is an example of a typical WPS research data set, in which the distributions of the demographic variables do not match the U.S. Census targets.
* `census_match_input` matches the census targets in its demographic composition.

We initialize several character vectors that will be used throughout the script.

###### RUNNABLE CODE
```{r read-data, eval = T}
library(here)
library(tidyverse)
library(survey)

set.seed(123)

var_order <- c("age", "age_range", "gender", "educ", "ethnic", "region", "clin_status")

var_order_census_match  <- c("gender", "educ", "ethnic", "region")

cat_order <- c(
  NA, "5", "6", "7", "8", "9", "10", "11", "12",
  NA, "5 to 8 yo", "9 to 12 yo", 
  NA, "male", "female",
  NA, "no_HS", "HS_grad", "some_college", "BA_plus",
  NA, "hispanic", "asian", "black", "white", "other",
  NA, "northeast", "south", "midwest", "west")

urlRemote_path  <- "https://raw.github.com/"
github_path <- "wpspublish/DSHerzberg-WEIGHTING-DATA/master/INPUT-FILES/"
fileName_path   <- "unweighted-input.csv"

original_input <- suppressMessages(read_csv(url(
  str_c(urlRemote_path, github_path, fileName_path)
)))

fileName_path   <- "data-input-sim.csv"

census_match_input <- suppressMessages(read_csv(url(
  str_c(urlRemote_path, github_path, fileName_path)
)))

rm(list = ls(pattern = "_path"))
```

<br>

###### COMMENTED SNIPPETS
Because some downstream procedures invoke R's random number generator, we use `base::set.seed()` to ensure that we obtain the same numerical results every time we run the script.
```{r read-data, echo = 6, eval = F}
```
The next snippet shows a method for reading data from a remote server. This makes the script more portable, because it does not depend on reading data stored on a local machine. The remote file path is divided into three segements (`urlRemote_path`, `github_path`, `fileName_path`), which are initialized as character vectors. This modularization allows easy subsitution of different servers, file names, etc. In the call of `readr::read_csv(url())`, which reads in the data, the three file path segments are concatenated into a single URL, using `stringr::str_c()`.
```{r read-data, echo = 19:26, eval = F}
```
The script removes unneeded objects from the environment using `base::rm()`, which takes as its argument a `list` of objects to remove. `base::ls()` returns the names of objects, here limited to those whose names have a `pattern` match with the substring `"_path"`. In this way we remove the file path name segments, which are no longer needed once the data are read in.
```{r read-data, echo = 33, eval = F}
```

<br>

#### B. Calculate demographic weighting multipliers using the `survey` package

The `survey` package provides the functions needed to implement demographic weighting multipliers as described in the _Overview_ section. Running the next code section yields a data frame in which each case is paired with a multiplier that reflects the level of discrepancy between the existing sample size of its demographic cell, and the sample size needed to meet the census target.

###### RUNNABLE CODE
```{r weights, eval = T, message = F}
census_match_cat_count <- var_order_census_match %>%
  map_df(
    ~
      census_match_input %>%
      group_by(across(all_of(.x))) %>%
      summarize(n_census = n()) %>%
      rename(cat = all_of(.x)) %>%
      mutate(var = all_of(.x),
             pct_census = n_census/10) %>%
      relocate(var, .before = cat)
  ) %>% 
  arrange(match(cat, cat_order))

var_order_census_match %>%
  map(
    ~ census_match_cat_count %>%
      filter(var == all_of(.x)) %>%
      select(-var, -pct_census) %>%
      rename(!!.x := cat, Freq = n_census)
  ) %>%
  setNames(str_c(var_order_census_match, "_census")) %>%
  list2env(envir = .GlobalEnv) %>% 
  invisible(.)

unweighted_survey_object <- svydesign(ids = ~1, 
                                      data = original_input, 
                                      weights = NULL)

rake_original_input <- rake(design = unweighted_survey_object,
                              sample.margins = list(~gender, ~educ, ~ethnic, ~region),
                              population.margins = list(gender_census, educ_census, 
                                                        ethnic_census, region_census))

input_demo_wts <- bind_cols(
  rake_original_input[["variables"]],  
  data.frame(rake_original_input[["prob"]]), 
  data.frame(demo_wt = weights(rake_original_input))
) %>% 
  rename(samp_prob = rake_original_input...prob...) %>% 
  mutate(ratio = samp_prob / demo_wt) %>% 
  select(ID:clin_status, samp_prob, demo_wt, ratio, everything()) %>% 
  arrange(desc(samp_prob))

rm(list = ls(pattern = "object|rake"))
```


<br>

###### COMMENTED SNIPPETS
The `survey` package requires as input the census target counts for the four key demographic variables. We can obtain these counts from `census_match_input`, one of the data files read in upstream, because it has the same number of cases as `original_input` (the file to which we will apply demographic weighting multipliers).

In the next snippet, we use `purrr::map_df()` to iterate over the character vector `var_order_census_match`, which contains the names of the four demographic variables. `map_df()` returns a data frame as output. The code within `map_df()` will be executed once for each element of `var_order_census_match`, and that element is substituted into the code wherever the token `.x` appears.

Within `map_df()`, an anonymous function is defined wherein `census_match_input` is piped into `dplyr::group_by()`. The use of `dplyr::across()` in this context allows us to group the data object by the demographic variable identified by the current value of `.x`, without having to introduce the complexities of non-standard evaluation (NSE) and unquoting. `tidyselect::all_of()` is a helper that allows `across()` (and other selecting functions) to select variables contained in a character vector.

Once the data are grouped by demographic variable, we can use `dplyr::summarize()` and `dplyr::n()` to create a summary table that gives the count of cases in the categories of each demographic variable (e.g., within the _variable_ `gender`, the _categories_ are `male` and `female`). The table is built sequentially as the function iterates over the elements of `var_order_census_match`, and the output is bound into a single data frame by `map_df`.

The remainder of this snippet formats the summary table. `dplyr::rename()` applies a uniform name `cat` to the column holding the categories within each demographic variable, `dplyr::mutate` creates two new columns (`var` for the names of the demographic variables, and `pct_census` for the census target percentages), `dplyr::relocate()` reorders the columns, and `dplyr::arrange()` sorts the table by `cat` to `match()` an order given by the character vector `cat_order`.
```{r weights, echo = 1:12, eval = F}
```
To prepare the census target counts for the `survey` package, we need to split the data frame containing these counts into four separate data frames, one for each demographic variable. Once again, we use `map()` to iterate over the names of the demographic variables, which are contained in `var_order_census_match`.

Within `map()`, an anonymous function is defined wherein the table of census target counts `census_match_cat_counts` created in the previous snippet is piped into `dplyr::filter()`, which selects only the rows for which the predicate `var == all_of(.x)` returns `TRUE`. That is, it selects rows where `var` is equal to the current element of `census_match_cat_counts`, designated by `.x`. We then remove the `var` and `pct_census` columns from the data object using `select(-var, -pct_census)`, and we `rename()` the two remaining columns. In order rename column `cat` to `.x`, we must unquote the `.x` element using `!!` and then employ the NSE operator `:=` instead of the conventional `=` sign.

`map()` returns a list whose elements are the four separate data frames containing census target counts. We name the elements of this list using `stats::setNames()`. The names are drawn from a four-element character vector that combines the elements of `var_order_census_match` with the suffix `"_census"`, concatenated with `str_c()`. Then we use `base::list2env()` to extract the four data frames into the global environment.
```{r weights, echo = 14:23, eval = F}
```
To use the `survey` package, we first call `survey::svydesign()`, which processes the input data to create a survey object (here named `unweighted_survey_object`). The survey object is a list that includes the input data frame (via the argument `data = original_input`) and other R objects needed by various functions in the `survey` package. `ids =` is a required argument specifying clusters in the input (here the forumula `~1` indicates no clusters). `weights = NULL` indicates that there are no pre-existing survey weights in the data.
```{r weights, echo = 25:27, eval = F}
```
Recall that our goal in using the `survey` package is to generate demographic weighting multipliers that allow us to adjust data from samples that do not match the demographic composition of the U.S. census. To do this we need to _rake_ our input data against the census target counts for the four demographic varibles. Raking (invoked by calling `survey::rake()`) is the analytic step by which the demographic proportions in the input data are fitted to the target census proportions.

Like `svydesign()`, `rake()` returns a list of objects that can be used by other `survey` functions. The first input to `rake()` is the survey design object created in the previous step, specified here by the argument `design = unweighted_survey_object`. The next input is the sample margins; that is, the unweighted demographic counts of the four variables of interest, from our input file `original_input`. This data frame is contained in `unweighted_survey_object`, and we extract the four counts with the argument `sample.margins = list(~gender, ~educ, ~ethnic, ~region)`, prepending the formula shorthand `~` to the name of each variable. The final input is the target census counts, or population margins. These counts are drawn from the four separate data frames that we created upstream, via the argument `population.margins = list(gender_census, educ_census, ethnic_census, region_census)`.
```{r weights, echo = 29:32, eval = F}
```
We can now calculate the demographic weighting multipliers for each case in our input sample. The `survey` package derives a unique multiplier for each possible crossing of the four demographic variables. There are 160 possible crossings (2 `gender` categories X 4 `educ` categories X 5 `ethnic` categories X 4 `region` categories), and each case is classified into one and only one crossing. Each crossing is associated with a unique demographic weighting multiplier, which is a function of the difference between the unweighted input count of that crossing (e.g., how many indviduals in the `female X high-school diploma X hispanic X south region` cell), and the target count from the U.S. census.

We can create a new data frame `input_demo_wts` with case-wise demographic multipliers using `dplyr::bind_cols()`, which joins input columns side-by-side, without matching by an index term. We start with the input data set, which we pluck from the raked object `rake_original_input` by subsetting `[[]]` the element `"variables"`. To this we bind the `"prob"` element from `rake_original_input`, which we convert into a column by wrapping it in `base::data.frame()`. The `"prob"` column holds the sampling probabilities associated with the input demographic counts, which are not themselves weighting multipliers, but are an expression of the divergence between the input sample demographics and the census targets. Under-sampled categories have sampling probabilities less than 1, and over-sampled categories have a sampling probabilities greater than 1. Finally, we bind a new column `demo_wt` that holds the actual case-wise weighting multipliers. These are extracted from `rake_original_input` by calling `survey::weights()`.

The remainder of this snippet restructures, relabels, and sorts this new data frame `input_demo_wts` containing the case-wise weighting multiplers, by applying `dplyr` functions as described previously. It then cleans up the environment by removing unneeded objects. Within `rm()`, the `pattern =` argument contains string elements separated by the logical _or_ operator `|`, so that objects whose names contain _any_ of these elements will be removed.
```{r weights, echo = 34:44, eval = F}
```

<br>

#### C. Apply multipliers to input data, write files for downstream analysis

This section creates separate output files for unweighted and weighted data.

###### RUNNABLE CODE

```{r apply-weights, eval = T}
unweighted_output <- input_demo_wts %>% 
  select(-c(samp_prob, ratio)) %>%
  rename_with(~ str_c("i", str_pad(
    as.character(1:50), 2, side = "left", pad = "0"), "_uw"), 
    i01:i50) %>% 
  mutate(
    TOT_raw_unweight = rowSums(.[grep("*_uw", names(.))]
  )) %>%
  relocate(TOT_raw_unweight, .after = demo_wt)

write_csv(
  unweighted_output,
  here(
    "OUTPUT-FILES/unweighted-data-for-analysis.csv" 
  ),
  na = ""
)

weighted_output <- original_input %>%
  left_join(unweighted_output[c("ID", "demo_wt")], by = "ID") %>%
  rename_with(~ str_c("i", str_pad(
    as.character(1:50), 2, side = "left", pad = "0"
  ), "_w"),
  i01:i50) %>%
  mutate(across(c(i01_w:i50_w),
                ~ . * demo_wt)) %>%
  mutate(
    TOT_raw_weight = rowSums(.[grep("*_w$", names(.))]
    )) %>%
  relocate(demo_wt, TOT_raw_weight, .before = i01_w)

write_csv(
  weighted_output,
  here(
    "OUTPUT-FILES/weighted-data-for-analysis.csv" 
  ),
  na = ""
)
```

<br>

###### COMMENTED SNIPPETS
It may be useful for some downstream analyses to have access to the original unweighted input data. We initialize `unweighted_output` to hold these data, starting the pipeline with `input_demo_weights`. We use `select()` to drop columns not needed in the output
```{r apply-weights, echo = 1:2, eval = F}
```
We use `dplyr::rename_with()` to rename the item columns. The first argument of `rename_with()` is a function (designated with `~`) that applies new names to a certain subset of columns. Here, we start with `as.character(1:50)` which coerces the sequence of integers from 1 to 50 into a character vector (so that we can use string functions with those elements). We use `stringr::str_pad()` to pad the elements with zeros `pad = "0"` on the `side = "left"`, so that all elements are represented by two characters (e.g., 01, 02, 03, etc.), This expression is then wrapped in `stringr::str_c()`, to prepend `"i"` and append `"_uw"`, the later indicating that the item scores are unweighted. The second argument of `rename_with()` specifies that `i01:i50` are the columns to be renamed.
```{r apply-weights, echo = 3:5, eval = F}
```
We use `mutate()` and `base::rowSums()` to caculate a total score `TOT_raw_unweight`, the sum of the 50 unweighted item scores. Within `rowSums()`, we use the `.[]` shorthand to select columns within the piped data object to sum. Within the single brackets, we pass a character vector of the names of the columns to be summed. We do this with the expression `grep("*_uw", names(.))`. `base::grep()` finds matches to a string pattern within a character vector. The first argument `"*_uw"` provides the pattern as a regular expression: any character string `*` ending in `_uw`. The second argument `names(.)` indicates that `grep()` will return matching names from the vector of column names of the piped data object. Thus we pass into `.[]` a vector of the item names, and these are the columns summed to yield `TOT_raw_unweight`. We then arrange the columns in the desired sequence using `relocate()`.
```{r apply-weights, echo = 6:9, eval = F}
```
We write the ouput to _.csv_ using `readr::write_csv()`, with the arugment `na = ""` indicating that any missing cells will be written as blanks.
```{r apply-weights, echo = 11:17, eval = F}
```
We now create a weighted data set `weighted_output` and write it to _.csv_ for analysis. We use `dplyr::left_join()` to join the `original_input` data frame to the case-wise demographic multipliers. The latter are passed to `left_join()` by the argument `unweighted_output[c("ID", "demo_wt")]`, which uses single brackets `[]` to subset a vector of named columns from `unweighted_output`. The argument `by = "ID"` designates a common indexing variable, ensuring proper alignment of the joined columns. We `rename_with()` the item columns using a function similar to that described previously, but this time appending `"_w"` to indicate that the item scores are weighted.
```{r apply-weights, echo = 18:24, eval = F}
```
We use two separate calls of `mutate()` to first apply weighting multipliers to the item scores, and then to calculate a weighted total raw score. Within `across()`, the target columns are specified `c(i01_w:i50_w)`, and a function `~` is applied, with the `.` shorthand indicating that the value of `demo_wt` is to be multiplied by the item score contained in each of the target columns. These weighted item scores are then processed by `rowSums()` to yield the total score `TOT_raw_weight`. Note the difference between how the same target columns are specified within `across()` and `rowSums()`. Because `across()` is a `tidyverse` function, it can use the transparent expression `i01_w:i50_w` to subset the columns, as opposed to `rowSums()`, which requires a variant of the `grep()` call described previously. In this instance, we append `$` to the regular expression, to indicate that the sought-after pattern must have no characters after `_w`. This properly excludes the column `demo_wt` from the calculation of `TOT_raw_weight`.
```{r apply-weights, echo = 25:30, eval = F}
```
<br>

### 3. Proof of Concept

Here we review the output of the demographic weighting method, to verify that the weighted item scores demonstrate expected properties.

We create a comparison table with the column `pct_diff`, which expresses, for each demographic variable and category, the difference between the input sample percentage and the census target percentage.
```{r cat-count-comp, eval = T, echo = F}
cat_count_comp <-
  var_order_census_match %>%
  map_df(
    ~
      original_input %>%
      group_by(across(all_of(.x))) %>%
      summarize(n_input = n()) %>%
      rename(cat = all_of(.x)) %>%
      mutate(var = all_of(.x),
             pct_input = n_input / 10) %>%
      relocate(var, .before = cat)
  ) %>%
  arrange(match(cat, cat_order)) %>%
  bind_cols(census_match_cat_count[c("n_census", "pct_census")]) %>%
  mutate(pct_diff = pct_input - pct_census)

knitr::kable(cat_count_comp %>%
               mutate(across(var,
                             ~ case_when(
                               lag(.x) == .x ~ "",
                               T ~ .x
                             ))),
             caption = "Table 1: Comparison of input sample percentage to census target")
```

Examining `pct_diff`, we can infer that the input sample percentage of cases in the `female X No_HS X hispanic X northeast` cell deviates further from its census target percentage than any other cell. In other words, it is the most under-sampled cell. We therefore also expect its weighting multiplier to be much greater than 1. Because of the discrepancy between the sample percentage and the census target, the cases in `female X No_HS X hispanic X northeast` require a relatively large degree of numerical adjustment to their item scores.

`Table 2` shows a single case from the `female X No_HS X hispanic X northeast` cell. The value of `demo_wt` is 7.37, meaning that all item scores of cases in this cell are multiplied by 7.37 to adjust for under-sampling relative to the census target.
```{r high-weight, eval = T, echo = F}
knitr::kable(
  weighted_output %>%
    filter(
      gender == "female" &
        educ == "no_HS" &
        ethnic == "hispanic" &
        region == "northeast"
    ) %>%
    select(-age_range,-clin_status,-(i01_w:i50_w),-TOT_raw_weight) %>%
    sample_n(1),
  digits = 2,
  caption = "Table 2: Demographic multiplier from under-sampled cell"
)
```

Analogously, we expect cells whose sample percentages closely approximate the census targets to have multipliers close to 1. Because they are neither under- nor over-sampled, those cells would need little adjustment to the numerical impact of their summed item scores.

Examining the `pct_diff` column of `Table 1` again, we can infer that the sample percentage associated with the `female X HS_grad X black X west` cell adheres closely to its census target. `Table 3` shows a single case from this cell. We see that that `demo_wt` is 0.98, close to 1 as expected.
```{r 1-weight, eval = T, echo = F}
knitr::kable(
  weighted_output %>%
    filter(
      gender == "female" &
        educ == "HS_grad" &
        ethnic == "black" &
        region == "west"
    ) %>%
    select(-age_range,-clin_status,-(i01_w:i50_w),-TOT_raw_weight) %>%
    sample_n(1),
  digits = 2,
  caption = "Table 3: Demographic mupltiplier from accurately sampled cell"
)
```


Now we inspect different sections of the data frame `input_demo_wts`, which combines the original input data with the caseswise sampling probability and weighting multiplier columns, and is sorted by sampling probability. The bottom (`tail`)
of this data frame contains cases from categories that were under-sampled in
the input, so we expect low `samp_prob` and high `demo_wt`.
```{r tail, eval = T, echo = F}
knitr::kable(
  tail(input_demo_wts) %>%
    select(-age_range, -clin_status, -(i01:i50), -ratio),
  digits = 2,
  caption = "Table 4: Cases from under-sampled cells"
)
```

Next we look at a small slice of the data frame containing cases from cells
whose sample percentages closely approximated their census targets. In this region, both `samp_prob` and `demo_wt` should be close to 1.
```{r middle, eval = T, echo = F}
knitr::kable(
  filter(input_demo_wts, between(samp_prob, .98, 1.02)) %>%
    select(-age_range, -clin_status, -(i01:i50), -ratio),
  digits = 2,
  caption = "Table 5: Cases from accurately sampled cells"
)
```

We examine the top (`head`) of the data frame, which contains cases from over-sampled cells Here, we expect to see high `samp_prob` and low `demo_wt`.
```{r head, eval = T, echo = F}
knitr::kable(
  head(input_demo_wts) %>%
    select(-age_range, -clin_status, -(i01:i50), -ratio),
  digits = 2,
  caption = "Table 6: Cases from over-sampled cells"
)
```

Plotting `samp_prob` against `demo_wt`, we visualize an inverse curvilinear relationship between the two variables that pivots around the value of 1, which represents an exact match between the input demographic counts and their census targets.
```{r plot-samp-prob-demo-wt, eval = T, echo = F}
ggplot(input_demo_wts, aes(demo_wt, samp_prob)) +
  geom_line(color = "darkblue", size = 1) +
  geom_point(x=1, y=1, color='purple', size = 3) + 
  scale_x_continuous(breaks = seq(0, 8, .5), minor_breaks = seq(0, 8, .1)) +
  scale_y_continuous(breaks = seq(0, 2.5, .5),
                     minor_breaks = seq(0, 2.5, .1)) +
  xlab("demographic weighting multiplier") +
  ylab("sampling probability") +
  annotate(
    "text",
    x = 1.5,
    y = 1.5,
    label = "Oversampled relative to census: sampling probability > 1",
    color = "red",
    hjust = 0
  ) +
  annotate(
    "text",
    x = 1.5,
    y = 1.4,
    label = "Undersampled relative to census: sampling probability < 1",
    color = "darkgreen",
    hjust = 0
  ) +
annotate(
  "text",
  x = 1.1,
  y = 1.1,
  label = "samp prob = weight = 1: input cell pct matches census pct",
  color = "purple",
  hjust = 0
) 
```

Next we create a table <!--`TOT_sum_cat_count_comp`-->that summarizes weighted and unweighted total scores per case, across the 15 subordinate categories of the four demogrpahic variables.
```{r TOT, eval = T, echo = F}
unweighted_TOT_sum <- var_order_census_match %>%
  map_df(
    ~
      unweighted_output %>%
      group_by(across(all_of(.x))) %>%
      summarize(TOT_sum_input = sum(TOT_raw_unweight)) %>% 
      rename(cat = all_of(.x)) %>%
      mutate(var = all_of(.x)) %>%
      relocate(var, .before = cat)
  ) %>% 
  arrange(match(cat, cat_order))

weighted_TOT_sum <- var_order_census_match %>%
  map_df(
    ~
      weighted_output %>%
      group_by(across(all_of(.x))) %>%
      summarize(TOT_sum_weighted = round(sum(TOT_raw_weight))) %>% 
      rename(cat = all_of(.x)) %>%
      mutate(var = all_of(.x)) %>%
      relocate(var, .before = cat)
  ) %>% 
  arrange(match(cat, cat_order))

list_comp <- list(census_match_cat_count[c("var", "cat", "n_census")], 
                  cat_count_comp[c("var", "cat", "n_input")],
                   weighted_TOT_sum, unweighted_TOT_sum)

TOT_sum_cat_count_comp <- list_comp %>%
  reduce(left_join, by = c("var", "cat")) %>%
  mutate(n_diff = n_input - n_census,
         sum_diff = TOT_sum_input - TOT_sum_weighted) %>%
  relocate(var,
           cat,
           n_input,
           n_census,
           TOT_sum_input,
           TOT_sum_weighted,
           n_diff,
           sum_diff) %>%
  mutate(cat = factor(cat, levels = cat))

knitr::kable(TOT_sum_cat_count_comp %>%
               mutate(across(var,
                             ~ case_when(
                               lag(.x) == .x ~ "",
                               T ~ .x
                             ))),
             caption = "Table 7: Comparison of unweighted and weighted total scores")
```

In each category, `n_diff` expresses the difference between the input sample size and the census target, and `sum_diff` captures the difference between the sum of total scores based on unweighted input data, and the sum based on weighted, census-adjusted item scores.

We find the expected relationship between these two variables. In under-sampled categories (e.g., `educ = No_HS`), where `n_diff` is negative, we observe that `sum_diff` is also negative. In under-sampled categories, the weighted sum of total scores needs to be greater than the unweighted sum, to compensate for the reduced numerical impact of the unweighted scores (which in turn results from a fewer-than required number of cases in that category). An analogous relationship is observed in over-sampled categories (e.g., `ethnic = white`).

Across categories, we see that the absolute magnitude of `sum_diff` varies as expected with `n_diff`, such that categories with the greatest level of deviance from their census targets also have the greatest disparity between weighted and unweighted sums of total scores.

We can plot the relationship between `n_diff` and `sum_diff` separately for each demographic variable. The graphs show a near-perfect correlation between the extent of mis-sampling (`n_diff`) and the magnitude of numerical correction applied to item scores by the weighting procedure (`sum_diff`).

```{r plot, eval = T, echo = F}
plot_data <- TOT_sum_cat_count_comp %>%
  mutate(across(var,
                ~ runner::fill_run(.)))

ggplot(plot_data, aes(n_diff, sum_diff)) +
  geom_point(aes(shape = var, color = cat), size = 3) +
  facet_wrap( ~ var) +
  xlab("n_diff") +
  ylab("sum_diff") +
  guides(col = guide_legend(nrow = 11)) +
  geom_smooth(
    method = 'lm',
    se = F,
    formula = y ~ x,
    size = .3
  ) +
  ggpmisc::stat_poly_eq(
    formula = y ~ x,
    aes(label = paste(..rr.label.., sep = '*plain(\',\')~')),
    rr.digits = 5,
    parse = TRUE
  )
```
