---
title: "Weighting data to adjust for demographically non-representative samples"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 1. Overview

To develop norms for tests and behavior rating scales, we collect nationwide samples comprised of data from many thousands of persons. Our intent is to have the demographic makeup of these samples match the U.S. census proportions, as closely as possible. The demographic variables of interest are age, gender, (parent) education level, race/ethnicity, and U.S. geographic region.

Our approach is to stratify the sample by age, and ensure that we have enough cases within each age group to develop stable norms. Within each age stratum, we attempt to match the proportions of gender, education level, race/ethnicity, and region to the census proportions for that age group. Inevitably, some demographic cells are harder to fill than others, especially lower levels of education and certain ethnic minority groups. Pursuing the last few cases to fill out these cells is expensive and time-consuming.

We now have the option of weighting the data of samples that fall short of the ideal demographic composition. We can calculate weighting multipliers corresponding to each possible crossing of the four key demographic variables. For example, one crossing would be _female_ X _high-school diploma_ X _hispanic_ X _south region_. All cases falling into that bucket would have the same weighting multiplier.

We can apply that multiplier to the data at the most granular level. As an example, we can use a hypothetical behavior rating scale where a parent rates 50 items about their child's level of hyperactive/inattentive behavior during the past month. These behaviors are rated on a frequency scale, coded as follows:

* Never (or almost never) = 1
* Occasionally = 2
* Frequently = 3
* Always (or almost always) = 4

A higher code on any item indicates greater frequency of problematic behavior.

We can apply a demographic weighting multiplier on a case-wise basis to each of the 50 item codes. These data would then be weighted for all subsequent item- and score-level analyses.

In practice this would work as follows. Suppose that the previously mentioned demographic cell (_female_ X _high-school diploma_ X _hispanic_ X _south region_) was under-represented in our sample, with respect to U.S. census proportions. The demographic weighting multiplier would serve to increase the numerical impact of the insufficient number of cases in this cell. Thus, if we collected 40 cases in this cell, and the census required 60, the multiplier would be applied to increase the values of the item codes in our 40 cases. Once these values were increased, summing the item codes from from our 40 cases would yield a sum approximately equivalent to the sum of the item codes of 60 non-weighted cases with the same demographic characteristics. 

To recount, we can use this weighting method to adjust the values of the data in our collected sample, so that the sample statistics would be similar to those obtained from a sample that exactly matched the U.S. Census demographic proportions.

### 2. Demonstration of Method

#### Load packages, specify input parameters, read data

We read in two simulated data sets, each including 1000 cases with the four demographic variables of interest, and 50 behavior rating items coded as described above. 

* `original_input` is an example of a typical WPS research data set, in which the distributions of the demographic variables do not match the U.S. Census targets.
* `census_match_input` matches the census targets in its demographic composition.

We set input parameters that will be re-used througout the script.

###### RUNNABLE CODE
```{r read-data, eval = F}
suppressMessages(library(here))
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(survey)))

set.seed(123)

var_order <- c("age", "age_range", "gender", "educ", "ethnic", "region", "clin_status")

var_order_census_match  <- c("gender", "educ", "ethnic", "region")

cat_order <- c(
  NA, "5", "6", "7", "8", "9", "10", "11", "12",
  NA, "5 to 8 yo", "9 to 12 yo", 
  NA, "male", "female",
  NA, "no_HS", "HS_grad", "some_college", "BA_plus",
  NA, "hispanic", "asian", "black", "white", "other",
  NA, "northeast", "south", "midwest", "west")

urlRemote_path  <- "https://raw.githubusercontent.com/"
github_path <- "DSHerzberg/WEIGHTING-DATA/master/INPUT-FILES/"
fileName_path   <- "unweighted-input.csv"

original_input <- suppressMessages(read_csv(url(
  str_c(urlRemote_path, github_path, fileName_path)
)))

fileName_path   <- "data-input-sim.csv"

census_match_input <- suppressMessages(read_csv(url(
  str_c(urlRemote_path, github_path, fileName_path)
)))

rm(list = ls(pattern = "_path"))
```

<br>

###### COMMENTED SNIPPETS
Because some downstream procedures invoke R's random number generator, we use `base::set.seed()` to ensure that we obtain the same numerical results every time we run the script.
```{r read-data, echo = 5, eval = F}
```
The next snippet shows a method for reading data from a remote server. This makes the script more portable, because it does not depend on reading data stored on a local machine. The remote file path is divided into three segements (`urlRemote_path`, `github_path`, `fileName_path`), which are initialized as character vectors. This modularization allows easy subsitution of different servers, file names, etc. In the call of `readr::read_csv(url())`, which reads in the data, the three file path segments are pasted together into a single URL, using `stringr::str_c()`.
```{r read-data, echo = 18:25, eval = F}
```
The script removes unneeded objects from the environment using `base::rm()`, which takes as its argument a `list` of objects to remove. `base::ls()` returns the names of objects, here limited to those whose names have a `pattern` match with the substring `"_path"`. In this way we remove the file path name segments, which are no longer needed once the data are read in.
```{r read-data, echo = 33, eval = F}
```

<br>

#### Calculate demographic weighting multipliers using the `survey` package

The `survey` package provides the functions needed to implement demographic weighting multipliers as described in the _Overview_ section. Running the next code section yields a data frame in which each case is paired with a muliplier that reflects the level of divergence between the distribution of its demographic cell in the input data, and the target distributions from the U.S. Census.

###### RUNNABLE CODE
```{r weights, eval = F}
census_match_cat_count <- var_order_census_match %>%
  map_df(
    ~
      census_match_input %>%
      group_by(across(all_of(.x))) %>%
      summarize(n_census = n()) %>%
      rename(cat = all_of(.x)) %>%
      mutate(var = all_of(.x),
             pct_census = n_census/10) %>%
      relocate(var, .before = cat)
  ) %>% 
  arrange(match(cat, cat_order))

var_order_census_match %>%
  map(
    ~ census_match_cat_count %>%
      filter(var == all_of(.x)) %>%
      select(-var, -pct_census) %>%
      rename(!!.x := cat, Freq = n_census)
  ) %>%
  setNames(str_c(var_order_census_match, "_census")) %>%
  list2env(envir = .GlobalEnv)

unweighted_survey_object <- svydesign(ids = ~1, 
                                      data = original_input, 
                                      weights = NULL)

rake_original_input <- rake(design = unweighted_survey_object,
                              sample.margins = list(~gender, ~educ, ~ethnic, ~region),
                              population.margins = list(gender_census, educ_census, 
                                                        ethnic_census, region_census))

input_demo_wts <- bind_cols(
  rake_original_input[["variables"]],  
  data.frame(rake_original_input[["prob"]]), 
  data.frame(demo_wt = weights(rake_original_input))
) %>% 
  rename(samp_prob = rake_original_input...prob...) %>% 
  mutate(ratio = samp_prob / demo_wt) %>% 
  select(ID:clin_status, samp_prob, demo_wt, ratio, everything()) %>% 
  arrange(desc(samp_prob))

rm(list = ls(pattern = "object|rake"))
```


<br>

###### COMMENTED SNIPPETS
The `survey` package requires as input the census target counts for the four key demographic variables. We can obtain these counts from `census_match_input`, one of the data files read in upstream, because it has the same number of cases as `original_input` (the file needing demographic weights).

In the next snippet, we use `purrr::map_df()` to iterate over the character vector `var_order_census_match`, which contains the names of the four demographic variables. `map_df()` returns a data frame as output (instead of a list). The code within `map_df()` will be executed once for each element of `var_order_census_match`, and that element is substituted into the code wherever the token `.x` appears.

Within `map_df()`, an anonymous function is defined wherein `census_match_input` is piped into `dplyr::group_by()`. The use of `dplyr::across()` in this context allows us to group the data object by the demographic variable identified by the current value of `.x`, without having to introduce the complexities of non-standard evaluation (NSE) and unquoting. `tidyselect::all_of()` is a helper that allows `across()` (and other selecting functions) to select variables contained in a character vector.

Once the data are grouped by demographic variable, we can use `dplyr::summarize()` and `dplyr::n()` to create a summary table that gives the count of cases in the categories of each demographic variable. This is done sequentially as the function iterates over the elements of `var_order_census_match`, and the output is bound into a single data frame by `map_df`.

The remainder of this snippet formats the summary table. `dplyr::rename()` applies a uniform name `cat` to the column holding the categories within each demographic variable, `dplyr::mutate` creates two new columns (`var` for the names of the demographic variables, and `pct_census` for the census target percentages), `dplyr::relocate()` reorders the columns, and `dplyr::arrange()` sorts the table by `cat` to `match()` an order given by the character vector `cat_order`.
```{r weights, echo = 1:12, eval = F}
```
To prepare the census target counts for the `survey` package, we need to split the data frame containing these counts into four separate data frames, one for each demographic variable. Once again, we use `map()` to iterate over the names of the demographic variables, which are contained in `var_order_census_match`.

Within `map()`, an anonymous function is defined wherein the table of census target counts `census_match_cat_counts` created in the previous snippet is piped into `dplyr::filter()`, which selects only the rows for which the predicate `var == all_of(.x)` returns `TRUE`. That is, it selects rows where `var` is equal to the current element of `census_match_cat_counts`, designated by `.x`. We then remove the `var` and `pct_census` columns from the data object using `select(-var, -pct_census)`, and we `rename()` the two remaining columns. In order rename column `cat` to `.x`, we must unquote the `.x` element using `!!` and then employ the NSE operator `:=` instead of the conventional `=` sign.

`map()` returns a list whose elements are the four separate data frames containing census target counts. We name the elements of this list using `stats::setNames()`. The names are drawn from a four-element character vector that combines the elements of `var_order_census_match` with the suffix `"_census"`, concatenated with `str_c()`. Then we use `base::list2env()` to extract the four data frames into the global environment.
```{r weights, echo = 14:22, eval = F}
```
To use the `survey` package, we first call `survey::svydesign()`, which processes the input data to create a survey object (here named `unweighted_survey_object`). The survey object is a list that includes the input data frame (via the argument `data = original_input`) and other R objects needed by various functions in the `survey` package. `ids =` is a required argument specifying clusters in the input (here the forumula `~1` indicates no clusters). `weights = NULL` indicates that there are no pre-existing survey weights in the data.
```{r weights, echo = 24:26, eval = F}
```
Recall that our goal in using the `survey` package is to generate demographic weighting multipliers that allow us to adjust data from samples that do not match the demographic composition of the U.S. census. To do this we need to _rake_ our input data against the census target counts for the four demographic varibles. Raking (invoked by calling `survey::rake()`) is the analytic step by which the demographic proportions in the input data are fitted to the target census proportions.

Like `svydesign()`, `rake()` returns a list of objects that can be used by other `survey` functions. The first input to `rake()` is the survey design object created in the previous step, specified here by the argument `design = unweighted_survey_object`. The next input is the sample margins; that is, the unweighted demographic counts of the four variables of interest, from our input file `original_input`. This data frame is contained in `unweighted_survey_object`, and we extract the four counts with the argument `sample.margins = list(~gender, ~educ, ~ethnic, ~region)`, prepending the formula shorthand `~` to the name of each variable. The final input is the target census counts, or population margins. These counts are drawn from the four separate data frames that we created upstream, via the argument `population.margins = list(gender_census, educ_census, ethnic_census, region_census)`.
```{r weights, echo = 28:31, eval = F}
```
We can now calculate the demographic weighting multipliers for each case in our input sample. The `survey` package derives a unique multiplier for each possible crossing of the four demographic variables. There are 160 possible crossings (2 `gender` categories X 4 `educ` categories X 5 `ethnic` categories X 4 `region` categories), and each case is classified into one and only one crossing. Each crossing is associated with a unique demographic weighting multiplier, which is a function of the difference between the unweighted input count of that crossing (e.g., how many indviduals in _female_ X _high-school diploma_ X _hispanic_ X _south region_ cell), and the target count from the U.S. census.

We can create a new data frame `input_demo_wts` with case-wise demographic multipliers using `dplyr::bind_cols()`, which joins input columns side-by-side, without matching by an index term. We start with the input data set, which we pluck from the raked object `rake_original_input` by subsetting `[[]]` the element `"variables"`. To this we bind the `"prob"` element from `rake_original_input`, which we convert into a column by wrapping it in `base::data.frame()`. The `"prob"` column holds the sampling probabilities associated with the input demographic counts, which are not themselves weighting multipliers, but are an expression of the divergence between the input sample demographics and the census targets. Under-sampled categories have a sampling probabilities less than one, and over-sampled categories have a probability greater than 1. Finally, we bind a new column `demo_wt` that holds the actual case-wise weighting multipliers. These are extracted from `rake_original_input` by calling `survey::weights()`.

The remainder of this snippet restructures, relabels, and sorts this new data frame `input_demo_wts` containing the case-wise weighting multiplers, by applying `dplyr` functions as described previously. It then cleans up the environment by removing unneeded objects. Within `rm()`, the `pattern =` argument contains string elements separated by the logical _or_ operator `|`, so that objects whose names contain _any_ of these elements will be removed.
```{r weights, echo = 33:43, eval = F}
```

<br>

#### Apply multipliers to input data, write files for downstream analysis

This section creates separate output files for unweighted and weighted data.

###### RUNNABLE CODE

```{r apply-weights, eval = F}
unweighted_output <- input_demo_wts %>% 
  select(-c(samp_prob, ratio)) %>%
  rename_with(~ str_c("i", str_pad(
    as.character(1:50), 2, side = "left", pad = "0"), "_uw"), 
    i01:i50) %>% 
  mutate(
    TOT_raw_unweight = rowSums(.[grep("*_uw", names(.))]
  )) %>%
  relocate(TOT_raw_unweight, .after = demo_wt)

write_csv(
  unweighted_output,
  here(
    "OUTPUT-FILES/unweighted-data-for-analysis.csv" 
  ),
  na = ""
)

weighted_output <- original_input %>%
  left_join(unweighted_output[c("ID", "demo_wt")], by = "ID") %>%
  rename_with(~ str_c("i", str_pad(
    as.character(1:50), 2, side = "left", pad = "0"
  ), "_w"),
  i01:i50) %>%
  mutate(across(c(i01_w:i50_w),
                ~ . * demo_wt)) %>%
  mutate(
    TOT_raw_weight = rowSums(.[grep("*_w$", names(.))]
    )) %>%
  relocate(demo_wt, TOT_raw_weight, .before = i01_w)

write_csv(
  weighted_output,
  here(
    "OUTPUT-FILES/weighted-data-for-analysis.csv" 
  ),
  na = ""
)
```

<br>

###### COMMENTED SNIPPETS
It may be useful for some downstream analyses to have access to the original unweighted input data. We initialize `unweighted_output` to hold these data, starting the pipeline with `input_demo_weights`. We use `select()` to drop columns not needed in the output
```{r apply-weights, echo = 1:2, eval = F}
```
We use `dplyr::rename_with()` to rename the item columns. The first argument of `rename_with()` is a function (designated with `~`) that applies new names to a certain subset of columns. Here, we start with `as.character(1:50)` which coerces the sequence of integers from 1 to 50 into a character vector (so that we can use string functions with those elements). We use `stringr::str_pad()` to pad the elements with zeros `pad = "0"` on the `side = "left"`, so that all elements are represented by two characters (e.g., 01, 02, 03, etc.), This expression is then wrapped in `stringr::str_c()`, to prepend `"i"` and append `"_uw"`, the later indicating that the item scores are unweighted. The second argument of `rename_with()` specifies that `i01:i50` are the columns to be renamed.
```{r apply-weights, echo = 3:5, eval = F}
```
We use `mutate()` and `base::rowSums()` to caculate a total score `TOT_raw_unweight`, the sum of the 50 unweighted item scores. Within `rowSums()`, we use the `.[]` shorthand to select columns within the piped data object to sum. Within the single brackets, we pass a character vector of the names of the columns to be summed. We do this with the expression `grep("*_uw", names(.))`. `base::grep()` finds matches to a string pattern within a character vector. The first argument `"*_uw"` provides the pattern as a regular expression: any character string `*` ending in `_uw`. The second argument `names(.)` indicates that `grep()` will return matching names from the vector of column names of the piped data object. Thus we pass into `.[]` a vector of the item names, and these are the columns summed to yield `TOT_raw_unweight`. We then arrange the columns in the desired sequence using `relocate()`.
```{r apply-weights, echo = 6:9, eval = F}
```
We write the ouput to _.csv_ using `readr::write_csv()`, with the arugment `na = ""` indicating that any missing cells will be written as blanks.
```{r apply-weights, echo = 11:17, eval = F}
```
We now create a weighted data set `weighted_output` and write it to _.csv_ for analysis. We use `dplyr::left_join()` to join the `original_input` data frame to the case-wise demographic multipliers. The latter are passed to `left_join()` by the argument `unweighted_output[c("ID", "demo_wt")]`, which uses single brackets `[]` to subset a vector of named columns from `unweighted_output`. We can use `by = "ID"` to designate a common indexing variable, thus ensuring proper alignment of the joined columns. We `rename_with()` the item columns using a function similar to that described previously, but this time appending `"_w"` to indicate that the item scores are weighted.
```{r apply-weights, echo = 18:24, eval = F}
```
We use two separate calls of `mutate()` to first apply weighting multipliers to the item scores, and then to calculate a weighted total raw score. Within `across()`, the target columns are specified `c(i01_w:i50_w)`, and a function `~` is applied, with the `.` shorthand indicating that the value of `demo_wt` is to be multiplied by the item score contained in each of the target columns. These weighted item scores are then processed by `rowSums()` to yield the total score `TOT_raw_weight`. Note the difference between how the same target columns are specified within `across()` and `rowSums()`. Because `across()` is a `tidyverse` function, it can use the transparent expression `i01_w:i50_w` to subset the columns, as opposed to `rowSums()`, which requires the `grep()` call described previously. In this instance, we append `$` to the regular expression, to indicate that the sought-after pattern must have no characters after `_w`. This properly excludes the column `demo_wt` from the calculation of `TOT_raw_weight`.
```{r apply-weights, echo = 25:30, eval = F}
```
<br>

### 3. Proof of Concept

In the following section, we examine the output of the demographic weighting method, to verify that the weighted item scores demonstrate expected properties.

First, we create a comparison table `cat_count_comp` with the column `pct_diff`, which expresses, for each demographic variable and category, the difference between the input sample percentage and the census target percentage.
```{r cat-count-comp, eval = F}
cat_count_comp <- var_order_census_match %>%
  map_df(
    ~
      original_input %>%
      group_by(across(all_of(.x))) %>%
      summarize(n_sample = n()) %>%
      rename(cat = all_of(.x)) %>%
      mutate(var = all_of(.x),
             pct_sample = n_sample/10) %>%
      relocate(var, .before = cat)
  ) %>% 
  arrange(match(cat, cat_order)) %>% 
  bind_cols(census_match_cat_count[c("n_census", "pct_census")]) %>% 
  mutate(pct_diff = pct_sample - pct_census)
```
Examining `pct_diff`, we would expect the percentage associated with the crossing _female_ X _No_HS_ X _hispanic_ X _northeast_ to be the one that deviates the farthest from its census target percentage. That appears to be the most under-sampled crossing. We would therefore also expect the weighting multiplier associated with that crossing to be much greater than 1, because this crossing requires the highest level of weighting, given that its count falls farther short of the census target than any other crossing.

In the next snippet we filter a single row in the _female_ X _No_HS_ X _hispanic_ X _northeast_ crossing and display a subset of columns on the console. The value of `demo_wt` is 7.37, meaning that each item score for cases in this crossing is multiplied by that amount to adjust for under-sampling in this crossing, relative to the census target.
```{r high-weight, eval = F}
weighted_output %>% 
  filter(
    gender == "female" &
      educ == "no_HS" &
      ethnic == "hispanic" &
      region == "northeast"
  ) %>% 
  select(-age_range, -clin_status, -(i01_w:i50_w), -TOT_raw_weight) %>% 
  sample_n(1)
```
Analogously, we would expect crossings whose counts approximate the census targets to have multipliers close to 1. Because they are neither under- nor over-sampled, those crossings would need little adjustment to the numerical impact of their collective item scores.

Examining the `pct_diff` column of `cat_count_comp` again, we find that the count associated with the crossing _female_ X _HS_grad_ X _black_ X _west_ seems to adhere closely to its census target. Extracting a single case from this crossing and printing it to the console, we find that `demo_wt` is 0.978, close to 1 as expected.
```{r 1-weight, eval = F}
weighted_output %>% 
  filter(
    gender == "female" &
      educ == "HS_grad" &
      ethnic == "black" &
      region == "west"
  ) %>% 
  select(-age_range, -clin_status, -(i01_w:i50_w), -TOT_raw_weight) %>% 
  sample_n(1)
```
Now we can inspect different sections of `input_demo_wts`, which pairs the original input data with the sampling probability and weighting multiplier columns, and is sorted by sampling probability. The bottom (tail)
of this data frame contains cases from categories that were under-sampled in
the input, so they should have low `samp_prob` and high `demo_wt`.
```{r tail, eval = F}
tail(input_demo_wts) %>% 
  select(-(i01:i50))
```
Next we look at a small slice of the data frame containing cases from crossings
whose counts closely approximated their census targets. In this region, both `samp_prob` and `demo_wt` should be close to 1.
```{r middle, eval = F}
filter(input_demo_wts, between(samp_prob, .98, 1.02)) %>% 
  select(-(i01:i50))
```
Finally, we examine the top (head) of he data frame, which contains cases from over-sampled crossings. Here, we expect to see high `samp_prob` and low `demo_wt`.
```{r head, eval = F}
head(input_demo_wts) %>% 
  select(-(i01:i50))
```
Plotting `samp_prob` against `demo_wt`, we visualize a smooth, inverse relationship between the two variables that pivots around the value of 1, which represents an exact match between the input demographic counts and their census targets.
```{r plot-samp-prob-demo-wt, eval = F}
ggplot(input_demo_wts, aes(demo_wt, samp_prob)) +
  geom_line(color = "darkblue", size = 1) +
  geom_point(x=1, y=1, color='purple', size = 3) + 
  scale_x_continuous(breaks = seq(0, 8, .5), minor_breaks = seq(0, 8, .1)) +
  scale_y_continuous(breaks = seq(0, 2.5, .5),
                     minor_breaks = seq(0, 2.5, .1)) +
  xlab("demographic weighting multiplier") +
  ylab("sampling probability") +
  annotate(
    "text",
    x = 1.5,
    y = 1.5,
    label = "Oversampled relative to census: sampling probability > 1",
    color = "red",
    hjust = 0
  ) +
  annotate(
    "text",
    x = 1.5,
    y = 1.4,
    label = "Undersampled relative to census: sampling probability < 1",
    color = "darkgreen",
    hjust = 0
  ) +
annotate(
  "text",
  x = 1.1,
  y = 1.1,
  label = "samp prob = weight = 1: demo cell pct matches census pct",
  color = "purple",
  hjust = 0
) 
```
